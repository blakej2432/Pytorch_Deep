{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zDvjMX55nann"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 문자 단위 RNN\n",
        "\n"
      ],
      "metadata": {
        "id": "SYsbSr__naan"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RB8D1kI5nMO9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = 'apple'\n",
        "label_str = 'pple!'\n",
        "char_vocab = sorted(list(set(input_str+label_str)))\n",
        "vocab_size = len(char_vocab)\n",
        "print ('문자 집합의 크기 : {}'.format(vocab_size))"
      ],
      "metadata": {
        "id": "fciqJH6vni-V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d07d295e-0269-4e8f-b10a-d45326cbf181"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_size = vocab_size # 입력의 크기는 문자 집합의 크기\n",
        "hidden_size = 5\n",
        "output_size = 5\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "xqb1aW29oCvu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_to_index = dict((c, i) for i, c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
        "print(char_to_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRMggJDvoW3V",
        "outputId": "c9a60ff5-6dd5-4769-d2f5-176430b8669c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_char={}\n",
        "for key, value in char_to_index.items():\n",
        "    index_to_char[value] = key\n",
        "print(index_to_char)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVuSzL0zoa8H",
        "outputId": "8dce5183-018c-4d7b-d080-ce1c088faef5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data = [char_to_index[c] for c in input_str]\n",
        "y_data = [char_to_index[c] for c in label_str]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKpf8XC6ogv2",
        "outputId": "9ef9ba4a-31d8-4be5-ae64-03b0472bee69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 4, 4, 3, 2]\n",
            "[4, 4, 3, 2, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 배치 차원 추가\n",
        "# 텐서 연산인 unsqueeze(0)를 통해 해결할 수도 있었음.\n",
        "x_data = [x_data]\n",
        "y_data = [y_data]\n",
        "print(x_data)\n",
        "print(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-50lnN2om9r",
        "outputId": "88c0bbd9-7069-40bd-9ec4-899e85eae231"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1, 4, 4, 3, 2]]\n",
            "[[4, 4, 3, 2, 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
        "print(x_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RAoxl2aouBr",
        "outputId": "4febe77d-e0ca-407c-fee0-c36b5ec3729d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[0., 1., 0., 0., 0.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 0., 1.],\n",
            "       [0., 0., 0., 1., 0.],\n",
            "       [0., 0., 1., 0., 0.]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.eye(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlT3bu5eow7D",
        "outputId": "88c750e2-93b2-4c90-a512-4b8166004129"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcj73Mr8pGgY",
        "outputId": "a1e72d83-d868-4604-e8d4-3c03f686ecc3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-c1bfbd518a63>:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  X = torch.FloatTensor(x_one_hot)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMrMnZ7upWZG",
        "outputId": "890726bb-fa34-4e77-e693-aec8f88f8123"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
            "레이블의 크기 : torch.Size([1, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first=True) # RNN 셀 구현\n",
        "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
        "\n",
        "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "76N1gdgEpby6"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(input_size, hidden_size, output_size)"
      ],
      "metadata": {
        "id": "mb0kGLkqpu2M"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWFRwRRjqwhx",
        "outputId": "f9bb15ed-7ee9-45e1-fb67-3046ed751021"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9XaGalSq1wL",
        "outputId": "6ad96893-ad78-449a-c7ef-f22ceec41c0b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([5, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_FDDEFPq8V0",
        "outputId": "dfb6ab57-dbc0-4cf2-bb36-286650a990ea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5])\n",
            "torch.Size([5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "yRzdS0xMrGwH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X)\n",
        "    loss = criterion(outputs.view(-1, input_size), Y.view(-1)) # view를 하는 이유는 Batch 차원 제거를 위해\n",
        "    loss.backward() # 기울기 계산\n",
        "    optimizer.step() # 아까 optimizer 선언 시 넣어둔 파라미터 업데이트\n",
        "\n",
        "    # 아래 세 줄은 모델이 실제 어떻게 예측했는지를 확인하기 위한 코드.\n",
        "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
        "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
        "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true Y: \", y_data, \"prediction str: \", result_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVV9ZkgerLmR",
        "outputId": "c2d79b98-caf6-4dbc-ef65-00de51e1cf72"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss:  1.5847843885421753 prediction:  [[2 2 2 2 2]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  eeeee\n",
            "1 loss:  1.376948595046997 prediction:  [[4 4 4 4 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
            "2 loss:  1.2170974016189575 prediction:  [[4 4 4 2 4]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppep\n",
            "3 loss:  1.0138742923736572 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "4 loss:  0.8117397427558899 prediction:  [[4 4 4 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
            "5 loss:  0.6145950555801392 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "6 loss:  0.4474720358848572 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "7 loss:  0.34007176756858826 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "8 loss:  0.24937565624713898 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "9 loss:  0.17458918690681458 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "10 loss:  0.12567861378192902 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "11 loss:  0.09250877797603607 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "12 loss:  0.06751886010169983 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "13 loss:  0.04911382868885994 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "14 loss:  0.036466434597969055 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "15 loss:  0.02801758050918579 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "16 loss:  0.022229351103305817 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "17 loss:  0.018044322729110718 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "18 loss:  0.014860063791275024 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "19 loss:  0.012363111600279808 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "20 loss:  0.010383804328739643 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "21 loss:  0.00881186407059431 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "22 loss:  0.007561188191175461 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "23 loss:  0.006560762412846088 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "24 loss:  0.005753295961767435 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "25 loss:  0.005094566382467747 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "26 loss:  0.004551019985228777 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "27 loss:  0.004097726661711931 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "28 loss:  0.0037161591462790966 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "29 loss:  0.0033921629656106234 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "30 loss:  0.003115041647106409 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "31 loss:  0.002876528073102236 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "32 loss:  0.0026700503658503294 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "33 loss:  0.002490407321602106 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "34 loss:  0.0023333632852882147 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "35 loss:  0.0021954625844955444 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "36 loss:  0.002073958981782198 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "37 loss:  0.0019664124120026827 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "38 loss:  0.001870879321359098 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "39 loss:  0.0017857716884464025 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "40 loss:  0.0017096191877499223 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "41 loss:  0.0016412828117609024 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "42 loss:  0.0015797188971191645 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "43 loss:  0.001524239545688033 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "44 loss:  0.0014738466124981642 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "45 loss:  0.0014281135518103838 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "46 loss:  0.0013864937936887145 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "47 loss:  0.0013483932707458735 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "48 loss:  0.0013134796172380447 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "49 loss:  0.0012813725043088198 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "50 loss:  0.0012517865980044007 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "51 loss:  0.0012244844110682607 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "52 loss:  0.001199156977236271 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "53 loss:  0.0011755897430703044 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "54 loss:  0.0011536645470187068 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "55 loss:  0.0011331193381920457 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "56 loss:  0.0011139542330056429 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "57 loss:  0.0010959312785416842 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "58 loss:  0.001079003093764186 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "59 loss:  0.001063050702214241 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "60 loss:  0.0010479550110176206 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "61 loss:  0.0010336923878639936 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "62 loss:  0.0010200724937021732 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "63 loss:  0.00100719032343477 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "64 loss:  0.0009948795195668936 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "65 loss:  0.0009831398492679 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "66 loss:  0.0009719001245684922 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "67 loss:  0.0009611126151867211 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "68 loss:  0.0009507775539532304 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "69 loss:  0.0009407756151631474 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "70 loss:  0.0009311783942393959 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "71 loss:  0.0009219145285896957 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "72 loss:  0.0009130076505243778 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "73 loss:  0.0009043150348588824 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "74 loss:  0.0008959319675341249 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "75 loss:  0.0008877631044015288 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "76 loss:  0.0008798561175353825 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "77 loss:  0.0008721397025510669 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "78 loss:  0.0008646136266179383 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "79 loss:  0.0008572543156333268 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "80 loss:  0.0008501567062921822 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "81 loss:  0.0008431306341663003 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "82 loss:  0.000836318708024919 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "83 loss:  0.0008296259911730886 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "84 loss:  0.0008230283856391907 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "85 loss:  0.0008166451007127762 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "86 loss:  0.0008102855645120144 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "87 loss:  0.0008041641558520496 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "88 loss:  0.000798090361058712 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "89 loss:  0.0007920881034806371 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "90 loss:  0.0007862047059461474 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "91 loss:  0.0007803927874192595 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "92 loss:  0.0007747237686999142 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "93 loss:  0.0007690784987062216 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "94 loss:  0.0007635045913048089 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "95 loss:  0.0007580974488519132 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "96 loss:  0.0007527617854066193 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "97 loss:  0.0007474020821973681 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "98 loss:  0.0007421855116263032 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
            "99 loss:  0.0007369925151579082 prediction:  [[4 4 3 2 0]] true Y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "더 많은 데이터 활용"
      ],
      "metadata": {
        "id": "S3cauzb5tqxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")"
      ],
      "metadata": {
        "id": "yUmFpZAnsqGi"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "DUdN4wBBtutI",
        "outputId": "1a0ec8fc-0886-4b32-e63c-167b4f8503f3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"if you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_set = list(set(sentence)) # 중복을 제거한 문자 집합 생성\n",
        "char_dic = {c: i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩"
      ],
      "metadata": {
        "id": "k7uo_QOnt0uK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "char_dic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KJyA__zt59J",
        "outputId": "adfc1d0e-1233-4b47-c647-c20a4d7e31c5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'b': 0,\n",
              " '.': 1,\n",
              " 'y': 2,\n",
              " 'w': 3,\n",
              " 'i': 4,\n",
              " 'h': 5,\n",
              " 'd': 6,\n",
              " 'm': 7,\n",
              " 'f': 8,\n",
              " 'e': 9,\n",
              " 'c': 10,\n",
              " 'g': 11,\n",
              " ' ': 12,\n",
              " 'k': 13,\n",
              " 'n': 14,\n",
              " 'o': 15,\n",
              " ',': 16,\n",
              " 'p': 17,\n",
              " 'r': 18,\n",
              " 'a': 19,\n",
              " 't': 20,\n",
              " 'l': 21,\n",
              " \"'\": 22,\n",
              " 's': 23,\n",
              " 'u': 24}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dic_size = len(char_dic)\n",
        "print('문자 집합의 크기 : {}'.format(dic_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Og2pnfQrt8rW",
        "outputId": "9d669731-d1e3-496e-e451-93f53128911d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문자 집합의 크기 : 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼파라미터 설정\n",
        "hidden_size = dic_size\n",
        "sequence_length = 10  # 임의 숫자 지정\n",
        "learning_rate = 0.1"
      ],
      "metadata": {
        "id": "W6HePDDHuEm1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 구성\n",
        "x_data = []\n",
        "y_data = []\n",
        "\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x_data.append([char_dic[c] for c in x_str])  # x str to index\n",
        "    y_data.append([char_dic[c] for c in y_str])  # y str to index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFGI3D_yuzeX",
        "outputId": "ee18f141-ce57-40d5-b8f1-3bc3a5800c89"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 if you wan -> f you want\n",
            "1 f you want ->  you want \n",
            "2  you want  -> you want t\n",
            "3 you want t -> ou want to\n",
            "4 ou want to -> u want to \n",
            "5 u want to  ->  want to b\n",
            "6  want to b -> want to bu\n",
            "7 want to bu -> ant to bui\n",
            "8 ant to bui -> nt to buil\n",
            "9 nt to buil -> t to build\n",
            "10 t to build ->  to build \n",
            "11  to build  -> to build a\n",
            "12 to build a -> o build a \n",
            "13 o build a  ->  build a s\n",
            "14  build a s -> build a sh\n",
            "15 build a sh -> uild a shi\n",
            "16 uild a shi -> ild a ship\n",
            "17 ild a ship -> ld a ship,\n",
            "18 ld a ship, -> d a ship, \n",
            "19 d a ship,  ->  a ship, d\n",
            "20  a ship, d -> a ship, do\n",
            "21 a ship, do ->  ship, don\n",
            "22  ship, don -> ship, don'\n",
            "23 ship, don' -> hip, don't\n",
            "24 hip, don't -> ip, don't \n",
            "25 ip, don't  -> p, don't d\n",
            "26 p, don't d -> , don't dr\n",
            "27 , don't dr ->  don't dru\n",
            "28  don't dru -> don't drum\n",
            "29 don't drum -> on't drum \n",
            "30 on't drum  -> n't drum u\n",
            "31 n't drum u -> 't drum up\n",
            "32 't drum up -> t drum up \n",
            "33 t drum up  ->  drum up p\n",
            "34  drum up p -> drum up pe\n",
            "35 drum up pe -> rum up peo\n",
            "36 rum up peo -> um up peop\n",
            "37 um up peop -> m up peopl\n",
            "38 m up peopl ->  up people\n",
            "39  up people -> up people \n",
            "40 up people  -> p people t\n",
            "41 p people t ->  people to\n",
            "42  people to -> people tog\n",
            "43 people tog -> eople toge\n",
            "44 eople toge -> ople toget\n",
            "45 ople toget -> ple togeth\n",
            "46 ple togeth -> le togethe\n",
            "47 le togethe -> e together\n",
            "48 e together ->  together \n",
            "49  together  -> together t\n",
            "50 together t -> ogether to\n",
            "51 ogether to -> gether to \n",
            "52 gether to  -> ether to c\n",
            "53 ether to c -> ther to co\n",
            "54 ther to co -> her to col\n",
            "55 her to col -> er to coll\n",
            "56 er to coll -> r to colle\n",
            "57 r to colle ->  to collec\n",
            "58  to collec -> to collect\n",
            "59 to collect -> o collect \n",
            "60 o collect  ->  collect w\n",
            "61  collect w -> collect wo\n",
            "62 collect wo -> ollect woo\n",
            "63 ollect woo -> llect wood\n",
            "64 llect wood -> lect wood \n",
            "65 lect wood  -> ect wood a\n",
            "66 ect wood a -> ct wood an\n",
            "67 ct wood an -> t wood and\n",
            "68 t wood and ->  wood and \n",
            "69  wood and  -> wood and d\n",
            "70 wood and d -> ood and do\n",
            "71 ood and do -> od and don\n",
            "72 od and don -> d and don'\n",
            "73 d and don' ->  and don't\n",
            "74  and don't -> and don't \n",
            "75 and don't  -> nd don't a\n",
            "76 nd don't a -> d don't as\n",
            "77 d don't as ->  don't ass\n",
            "78  don't ass -> don't assi\n",
            "79 don't assi -> on't assig\n",
            "80 on't assig -> n't assign\n",
            "81 n't assign -> 't assign \n",
            "82 't assign  -> t assign t\n",
            "83 t assign t ->  assign th\n",
            "84  assign th -> assign the\n",
            "85 assign the -> ssign them\n",
            "86 ssign them -> sign them \n",
            "87 sign them  -> ign them t\n",
            "88 ign them t -> gn them ta\n",
            "89 gn them ta -> n them tas\n",
            "90 n them tas ->  them task\n",
            "91  them task -> them tasks\n",
            "92 them tasks -> hem tasks \n",
            "93 hem tasks  -> em tasks a\n",
            "94 em tasks a -> m tasks an\n",
            "95 m tasks an ->  tasks and\n",
            "96  tasks and -> tasks and \n",
            "97 tasks and  -> asks and w\n",
            "98 asks and w -> sks and wo\n",
            "99 sks and wo -> ks and wor\n",
            "100 ks and wor -> s and work\n",
            "101 s and work ->  and work,\n",
            "102  and work, -> and work, \n",
            "103 and work,  -> nd work, b\n",
            "104 nd work, b -> d work, bu\n",
            "105 d work, bu ->  work, but\n",
            "106  work, but -> work, but \n",
            "107 work, but  -> ork, but r\n",
            "108 ork, but r -> rk, but ra\n",
            "109 rk, but ra -> k, but rat\n",
            "110 k, but rat -> , but rath\n",
            "111 , but rath ->  but rathe\n",
            "112  but rathe -> but rather\n",
            "113 but rather -> ut rather \n",
            "114 ut rather  -> t rather t\n",
            "115 t rather t ->  rather te\n",
            "116  rather te -> rather tea\n",
            "117 rather tea -> ather teac\n",
            "118 ather teac -> ther teach\n",
            "119 ther teach -> her teach \n",
            "120 her teach  -> er teach t\n",
            "121 er teach t -> r teach th\n",
            "122 r teach th ->  teach the\n",
            "123  teach the -> teach them\n",
            "124 teach them -> each them \n",
            "125 each them  -> ach them t\n",
            "126 ach them t -> ch them to\n",
            "127 ch them to -> h them to \n",
            "128 h them to  ->  them to l\n",
            "129  them to l -> them to lo\n",
            "130 them to lo -> hem to lon\n",
            "131 hem to lon -> em to long\n",
            "132 em to long -> m to long \n",
            "133 m to long  ->  to long f\n",
            "134  to long f -> to long fo\n",
            "135 to long fo -> o long for\n",
            "136 o long for ->  long for \n",
            "137  long for  -> long for t\n",
            "138 long for t -> ong for th\n",
            "139 ong for th -> ng for the\n",
            "140 ng for the -> g for the \n",
            "141 g for the  ->  for the e\n",
            "142  for the e -> for the en\n",
            "143 for the en -> or the end\n",
            "144 or the end -> r the endl\n",
            "145 r the endl ->  the endle\n",
            "146  the endle -> the endles\n",
            "147 the endles -> he endless\n",
            "148 he endless -> e endless \n",
            "149 e endless  ->  endless i\n",
            "150  endless i -> endless im\n",
            "151 endless im -> ndless imm\n",
            "152 ndless imm -> dless imme\n",
            "153 dless imme -> less immen\n",
            "154 less immen -> ess immens\n",
            "155 ess immens -> ss immensi\n",
            "156 ss immensi -> s immensit\n",
            "157 s immensit ->  immensity\n",
            "158  immensity -> immensity \n",
            "159 immensity  -> mmensity o\n",
            "160 mmensity o -> mensity of\n",
            "161 mensity of -> ensity of \n",
            "162 ensity of  -> nsity of t\n",
            "163 nsity of t -> sity of th\n",
            "164 sity of th -> ity of the\n",
            "165 ity of the -> ty of the \n",
            "166 ty of the  -> y of the s\n",
            "167 y of the s ->  of the se\n",
            "168  of the se -> of the sea\n",
            "169 of the sea -> f the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siaDunx5u0xI",
        "outputId": "2bba1f79-9e9d-4212-dfc7-bf3ed684b202"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[4, 8, 12, 2, 15, 24, 12, 3, 19, 14],\n",
              " [8, 12, 2, 15, 24, 12, 3, 19, 14, 20],\n",
              " [12, 2, 15, 24, 12, 3, 19, 14, 20, 12],\n",
              " [2, 15, 24, 12, 3, 19, 14, 20, 12, 20],\n",
              " [15, 24, 12, 3, 19, 14, 20, 12, 20, 15],\n",
              " [24, 12, 3, 19, 14, 20, 12, 20, 15, 12],\n",
              " [12, 3, 19, 14, 20, 12, 20, 15, 12, 0],\n",
              " [3, 19, 14, 20, 12, 20, 15, 12, 0, 24],\n",
              " [19, 14, 20, 12, 20, 15, 12, 0, 24, 4],\n",
              " [14, 20, 12, 20, 15, 12, 0, 24, 4, 21],\n",
              " [20, 12, 20, 15, 12, 0, 24, 4, 21, 6],\n",
              " [12, 20, 15, 12, 0, 24, 4, 21, 6, 12],\n",
              " [20, 15, 12, 0, 24, 4, 21, 6, 12, 19],\n",
              " [15, 12, 0, 24, 4, 21, 6, 12, 19, 12],\n",
              " [12, 0, 24, 4, 21, 6, 12, 19, 12, 23],\n",
              " [0, 24, 4, 21, 6, 12, 19, 12, 23, 5],\n",
              " [24, 4, 21, 6, 12, 19, 12, 23, 5, 4],\n",
              " [4, 21, 6, 12, 19, 12, 23, 5, 4, 17],\n",
              " [21, 6, 12, 19, 12, 23, 5, 4, 17, 16],\n",
              " [6, 12, 19, 12, 23, 5, 4, 17, 16, 12],\n",
              " [12, 19, 12, 23, 5, 4, 17, 16, 12, 6],\n",
              " [19, 12, 23, 5, 4, 17, 16, 12, 6, 15],\n",
              " [12, 23, 5, 4, 17, 16, 12, 6, 15, 14],\n",
              " [23, 5, 4, 17, 16, 12, 6, 15, 14, 22],\n",
              " [5, 4, 17, 16, 12, 6, 15, 14, 22, 20],\n",
              " [4, 17, 16, 12, 6, 15, 14, 22, 20, 12],\n",
              " [17, 16, 12, 6, 15, 14, 22, 20, 12, 6],\n",
              " [16, 12, 6, 15, 14, 22, 20, 12, 6, 18],\n",
              " [12, 6, 15, 14, 22, 20, 12, 6, 18, 24],\n",
              " [6, 15, 14, 22, 20, 12, 6, 18, 24, 7],\n",
              " [15, 14, 22, 20, 12, 6, 18, 24, 7, 12],\n",
              " [14, 22, 20, 12, 6, 18, 24, 7, 12, 24],\n",
              " [22, 20, 12, 6, 18, 24, 7, 12, 24, 17],\n",
              " [20, 12, 6, 18, 24, 7, 12, 24, 17, 12],\n",
              " [12, 6, 18, 24, 7, 12, 24, 17, 12, 17],\n",
              " [6, 18, 24, 7, 12, 24, 17, 12, 17, 9],\n",
              " [18, 24, 7, 12, 24, 17, 12, 17, 9, 15],\n",
              " [24, 7, 12, 24, 17, 12, 17, 9, 15, 17],\n",
              " [7, 12, 24, 17, 12, 17, 9, 15, 17, 21],\n",
              " [12, 24, 17, 12, 17, 9, 15, 17, 21, 9],\n",
              " [24, 17, 12, 17, 9, 15, 17, 21, 9, 12],\n",
              " [17, 12, 17, 9, 15, 17, 21, 9, 12, 20],\n",
              " [12, 17, 9, 15, 17, 21, 9, 12, 20, 15],\n",
              " [17, 9, 15, 17, 21, 9, 12, 20, 15, 11],\n",
              " [9, 15, 17, 21, 9, 12, 20, 15, 11, 9],\n",
              " [15, 17, 21, 9, 12, 20, 15, 11, 9, 20],\n",
              " [17, 21, 9, 12, 20, 15, 11, 9, 20, 5],\n",
              " [21, 9, 12, 20, 15, 11, 9, 20, 5, 9],\n",
              " [9, 12, 20, 15, 11, 9, 20, 5, 9, 18],\n",
              " [12, 20, 15, 11, 9, 20, 5, 9, 18, 12],\n",
              " [20, 15, 11, 9, 20, 5, 9, 18, 12, 20],\n",
              " [15, 11, 9, 20, 5, 9, 18, 12, 20, 15],\n",
              " [11, 9, 20, 5, 9, 18, 12, 20, 15, 12],\n",
              " [9, 20, 5, 9, 18, 12, 20, 15, 12, 10],\n",
              " [20, 5, 9, 18, 12, 20, 15, 12, 10, 15],\n",
              " [5, 9, 18, 12, 20, 15, 12, 10, 15, 21],\n",
              " [9, 18, 12, 20, 15, 12, 10, 15, 21, 21],\n",
              " [18, 12, 20, 15, 12, 10, 15, 21, 21, 9],\n",
              " [12, 20, 15, 12, 10, 15, 21, 21, 9, 10],\n",
              " [20, 15, 12, 10, 15, 21, 21, 9, 10, 20],\n",
              " [15, 12, 10, 15, 21, 21, 9, 10, 20, 12],\n",
              " [12, 10, 15, 21, 21, 9, 10, 20, 12, 3],\n",
              " [10, 15, 21, 21, 9, 10, 20, 12, 3, 15],\n",
              " [15, 21, 21, 9, 10, 20, 12, 3, 15, 15],\n",
              " [21, 21, 9, 10, 20, 12, 3, 15, 15, 6],\n",
              " [21, 9, 10, 20, 12, 3, 15, 15, 6, 12],\n",
              " [9, 10, 20, 12, 3, 15, 15, 6, 12, 19],\n",
              " [10, 20, 12, 3, 15, 15, 6, 12, 19, 14],\n",
              " [20, 12, 3, 15, 15, 6, 12, 19, 14, 6],\n",
              " [12, 3, 15, 15, 6, 12, 19, 14, 6, 12],\n",
              " [3, 15, 15, 6, 12, 19, 14, 6, 12, 6],\n",
              " [15, 15, 6, 12, 19, 14, 6, 12, 6, 15],\n",
              " [15, 6, 12, 19, 14, 6, 12, 6, 15, 14],\n",
              " [6, 12, 19, 14, 6, 12, 6, 15, 14, 22],\n",
              " [12, 19, 14, 6, 12, 6, 15, 14, 22, 20],\n",
              " [19, 14, 6, 12, 6, 15, 14, 22, 20, 12],\n",
              " [14, 6, 12, 6, 15, 14, 22, 20, 12, 19],\n",
              " [6, 12, 6, 15, 14, 22, 20, 12, 19, 23],\n",
              " [12, 6, 15, 14, 22, 20, 12, 19, 23, 23],\n",
              " [6, 15, 14, 22, 20, 12, 19, 23, 23, 4],\n",
              " [15, 14, 22, 20, 12, 19, 23, 23, 4, 11],\n",
              " [14, 22, 20, 12, 19, 23, 23, 4, 11, 14],\n",
              " [22, 20, 12, 19, 23, 23, 4, 11, 14, 12],\n",
              " [20, 12, 19, 23, 23, 4, 11, 14, 12, 20],\n",
              " [12, 19, 23, 23, 4, 11, 14, 12, 20, 5],\n",
              " [19, 23, 23, 4, 11, 14, 12, 20, 5, 9],\n",
              " [23, 23, 4, 11, 14, 12, 20, 5, 9, 7],\n",
              " [23, 4, 11, 14, 12, 20, 5, 9, 7, 12],\n",
              " [4, 11, 14, 12, 20, 5, 9, 7, 12, 20],\n",
              " [11, 14, 12, 20, 5, 9, 7, 12, 20, 19],\n",
              " [14, 12, 20, 5, 9, 7, 12, 20, 19, 23],\n",
              " [12, 20, 5, 9, 7, 12, 20, 19, 23, 13],\n",
              " [20, 5, 9, 7, 12, 20, 19, 23, 13, 23],\n",
              " [5, 9, 7, 12, 20, 19, 23, 13, 23, 12],\n",
              " [9, 7, 12, 20, 19, 23, 13, 23, 12, 19],\n",
              " [7, 12, 20, 19, 23, 13, 23, 12, 19, 14],\n",
              " [12, 20, 19, 23, 13, 23, 12, 19, 14, 6],\n",
              " [20, 19, 23, 13, 23, 12, 19, 14, 6, 12],\n",
              " [19, 23, 13, 23, 12, 19, 14, 6, 12, 3],\n",
              " [23, 13, 23, 12, 19, 14, 6, 12, 3, 15],\n",
              " [13, 23, 12, 19, 14, 6, 12, 3, 15, 18],\n",
              " [23, 12, 19, 14, 6, 12, 3, 15, 18, 13],\n",
              " [12, 19, 14, 6, 12, 3, 15, 18, 13, 16],\n",
              " [19, 14, 6, 12, 3, 15, 18, 13, 16, 12],\n",
              " [14, 6, 12, 3, 15, 18, 13, 16, 12, 0],\n",
              " [6, 12, 3, 15, 18, 13, 16, 12, 0, 24],\n",
              " [12, 3, 15, 18, 13, 16, 12, 0, 24, 20],\n",
              " [3, 15, 18, 13, 16, 12, 0, 24, 20, 12],\n",
              " [15, 18, 13, 16, 12, 0, 24, 20, 12, 18],\n",
              " [18, 13, 16, 12, 0, 24, 20, 12, 18, 19],\n",
              " [13, 16, 12, 0, 24, 20, 12, 18, 19, 20],\n",
              " [16, 12, 0, 24, 20, 12, 18, 19, 20, 5],\n",
              " [12, 0, 24, 20, 12, 18, 19, 20, 5, 9],\n",
              " [0, 24, 20, 12, 18, 19, 20, 5, 9, 18],\n",
              " [24, 20, 12, 18, 19, 20, 5, 9, 18, 12],\n",
              " [20, 12, 18, 19, 20, 5, 9, 18, 12, 20],\n",
              " [12, 18, 19, 20, 5, 9, 18, 12, 20, 9],\n",
              " [18, 19, 20, 5, 9, 18, 12, 20, 9, 19],\n",
              " [19, 20, 5, 9, 18, 12, 20, 9, 19, 10],\n",
              " [20, 5, 9, 18, 12, 20, 9, 19, 10, 5],\n",
              " [5, 9, 18, 12, 20, 9, 19, 10, 5, 12],\n",
              " [9, 18, 12, 20, 9, 19, 10, 5, 12, 20],\n",
              " [18, 12, 20, 9, 19, 10, 5, 12, 20, 5],\n",
              " [12, 20, 9, 19, 10, 5, 12, 20, 5, 9],\n",
              " [20, 9, 19, 10, 5, 12, 20, 5, 9, 7],\n",
              " [9, 19, 10, 5, 12, 20, 5, 9, 7, 12],\n",
              " [19, 10, 5, 12, 20, 5, 9, 7, 12, 20],\n",
              " [10, 5, 12, 20, 5, 9, 7, 12, 20, 15],\n",
              " [5, 12, 20, 5, 9, 7, 12, 20, 15, 12],\n",
              " [12, 20, 5, 9, 7, 12, 20, 15, 12, 21],\n",
              " [20, 5, 9, 7, 12, 20, 15, 12, 21, 15],\n",
              " [5, 9, 7, 12, 20, 15, 12, 21, 15, 14],\n",
              " [9, 7, 12, 20, 15, 12, 21, 15, 14, 11],\n",
              " [7, 12, 20, 15, 12, 21, 15, 14, 11, 12],\n",
              " [12, 20, 15, 12, 21, 15, 14, 11, 12, 8],\n",
              " [20, 15, 12, 21, 15, 14, 11, 12, 8, 15],\n",
              " [15, 12, 21, 15, 14, 11, 12, 8, 15, 18],\n",
              " [12, 21, 15, 14, 11, 12, 8, 15, 18, 12],\n",
              " [21, 15, 14, 11, 12, 8, 15, 18, 12, 20],\n",
              " [15, 14, 11, 12, 8, 15, 18, 12, 20, 5],\n",
              " [14, 11, 12, 8, 15, 18, 12, 20, 5, 9],\n",
              " [11, 12, 8, 15, 18, 12, 20, 5, 9, 12],\n",
              " [12, 8, 15, 18, 12, 20, 5, 9, 12, 9],\n",
              " [8, 15, 18, 12, 20, 5, 9, 12, 9, 14],\n",
              " [15, 18, 12, 20, 5, 9, 12, 9, 14, 6],\n",
              " [18, 12, 20, 5, 9, 12, 9, 14, 6, 21],\n",
              " [12, 20, 5, 9, 12, 9, 14, 6, 21, 9],\n",
              " [20, 5, 9, 12, 9, 14, 6, 21, 9, 23],\n",
              " [5, 9, 12, 9, 14, 6, 21, 9, 23, 23],\n",
              " [9, 12, 9, 14, 6, 21, 9, 23, 23, 12],\n",
              " [12, 9, 14, 6, 21, 9, 23, 23, 12, 4],\n",
              " [9, 14, 6, 21, 9, 23, 23, 12, 4, 7],\n",
              " [14, 6, 21, 9, 23, 23, 12, 4, 7, 7],\n",
              " [6, 21, 9, 23, 23, 12, 4, 7, 7, 9],\n",
              " [21, 9, 23, 23, 12, 4, 7, 7, 9, 14],\n",
              " [9, 23, 23, 12, 4, 7, 7, 9, 14, 23],\n",
              " [23, 23, 12, 4, 7, 7, 9, 14, 23, 4],\n",
              " [23, 12, 4, 7, 7, 9, 14, 23, 4, 20],\n",
              " [12, 4, 7, 7, 9, 14, 23, 4, 20, 2],\n",
              " [4, 7, 7, 9, 14, 23, 4, 20, 2, 12],\n",
              " [7, 7, 9, 14, 23, 4, 20, 2, 12, 15],\n",
              " [7, 9, 14, 23, 4, 20, 2, 12, 15, 8],\n",
              " [9, 14, 23, 4, 20, 2, 12, 15, 8, 12],\n",
              " [14, 23, 4, 20, 2, 12, 15, 8, 12, 20],\n",
              " [23, 4, 20, 2, 12, 15, 8, 12, 20, 5],\n",
              " [4, 20, 2, 12, 15, 8, 12, 20, 5, 9],\n",
              " [20, 2, 12, 15, 8, 12, 20, 5, 9, 12],\n",
              " [2, 12, 15, 8, 12, 20, 5, 9, 12, 23],\n",
              " [12, 15, 8, 12, 20, 5, 9, 12, 23, 9],\n",
              " [15, 8, 12, 20, 5, 9, 12, 23, 9, 19]]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
        "X = torch.FloatTensor(x_one_hot)\n",
        "Y = torch.LongTensor(y_data)"
      ],
      "metadata": {
        "id": "739R1wrxu_5E"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_one_hot)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ19bJCCvWN5",
        "outputId": "dd18ed3a-0608-46e6-85a4-2c7910e1f518"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "170"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_one_hot[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfENmS6wvYfZ",
        "outputId": "67c336c3-2c9c-41c9-ca11-651980c16e9e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "        0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "        0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('훈련 데이터의 크기 : {}'.format(X.shape))\n",
        "print('레이블의 크기 : {}'.format(Y.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hu41GW4qvhWq",
        "outputId": "430db99f-f0fb-43b5-b143-035b9a0ec27f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
            "레이블의 크기 : torch.Size([170, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eoXQPgTNvm-X",
        "outputId": "e8fcd13f-7224-4c88-94c0-b03d8ca02e42"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 8, 12,  2, 15, 24, 12,  3, 19, 14, 20])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layers): # 현재 hidden_size는 dic_size와 같음.\n",
        "        super(Net, self).__init__()\n",
        "        self.rnn = torch.nn.RNN(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "        self.fc = torch.nn.Linear(hidden_dim, hidden_dim, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, _status = self.rnn(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "1JDwuqoFvwd9"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = Net(dic_size, hidden_size, 2) # 이번에는 층을 두 개 쌓습니다.\n"
      ],
      "metadata": {
        "id": "7KeRUgzGv6OB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), learning_rate)"
      ],
      "metadata": {
        "id": "eOS7cr18wCqz"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = net(X)\n",
        "print(outputs.shape) # 3차원 텐서"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeOQqYZKwDwJ",
        "outputId": "3af7022c-97b5-4adb-92d5-3f1186a07d71"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZf3pQZqwFvz",
        "outputId": "fd9010f5-edea-46ac-e2b7-50ee6ece3de9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1700, 25])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Y.shape)\n",
        "print(Y.view(-1).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTOeOkcpwThe",
        "outputId": "39401b09-c07a-477f-83c1-9bb55f1f7e92"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([170, 10])\n",
            "torch.Size([1700])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크마다 모델의 입력으로 사용\n",
        "    loss = criterion(outputs.view(-1, dic_size), Y.view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # results의 텐서 크기는 (170, 10)\n",
        "    results = outputs.argmax(dim=2)\n",
        "    predict_str = \"\"\n",
        "    for j, result in enumerate(results):\n",
        "        if j == 0: # 처음에는 예측 결과를 전부 가져오지만\n",
        "            predict_str += ''.join([char_set[t] for t in result])\n",
        "        else: # 그 다음에는 마지막 글자만 반복 추가\n",
        "            predict_str += char_set[result[-1]]\n",
        "\n",
        "    print(predict_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anAoJhgWwZ_l",
        "outputId": "79461063-e3f8-466e-8734-1c77b4f1d45c"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "''shl'isssaihi's'hihihik'hiaihi'saihipl'hlspli'a'ihi'siiiiihi'sliiihiiplshihliihi'saihlkiaaia'il'hl'i'ihliisl's'ipslipll'iiihihs'ihiil'hi'hi's'hs'shiiiiliiiiiihls'''iiiihssh'iikih\n",
            "                                                                                                                                                                                   \n",
            "taaoaoaoaoatoaoaoaotthaoaoaoaoaoaoaoaoaottatoaoaotoaoaoaoaottoataetthaoaoaotehaoaoaoaoaotoaoteoattehthaoaoaoaoaoaoaoaoaoaoaoathaeaoaotttaoetttaoaoaoaoaoaoaoaoaoahaoetteotoaoaoaoao\n",
            "  t  t  t  ttttt t  t  t t   t t  t  t  tt t  t rt t  t   t  tt    t  ttt     t tt tt  t   t tt  t  t t  tt  t t t ttt ttt t t t t  t  t  t t  t tt t  tt  t   t t     tt     t  t \n",
            "    o  o            e      o   n o   n          e         e     o  r  o   o                           o  e                       e  n    en e  e   o   o     o     o          o  n \n",
            "      ie,i' ''i'ii'' l'i 'i 'i i' i'' ''i'i ' '' ''ii''i ' l'il' '' ''o'' oi''i'''l il'o''i'l i'oe'i 'ol' '' ' ii'' i'i''' ilii''olie'ii'o ' '' ''ioi''o''  'o' i'' 'ii  l'il e'i '\n",
            "       os sssssssssssssssssssssss ssssssssssssssssssssssssssssss ssssssssssssssssssssssssssssssss ssss ssssssssssssssss sssssssss sssssss ss ssssssssssssssss ssssssssss sssss ssss\n",
            "       to o o  ooooooo o oo oo ooo o  o ooooooooooo      o       ooo  o oo ooooo ooo o  oo oo oo     o   otoo oo ooo oo o o o  o     o oo oo  oo ooo oooooo o ooo ooooo  oo o  oooo\n",
            "   t  t  t        oo      t       t   o                      o o                     o  o  oo    t   o           o   o          o          o          o     o o  o      to    t   o\n",
            "  to  to t    t  t    o  o  oo  o tt   oo t ooto t o to     otoo to to  t  ot    to t   t  to t  t  o  o  to to too o ot o t    o     to  oo ao eoo oto   o   t   o  to t ot  to o \n",
            "o to  ton to ttootttott  t ttottto teottttt  eteot ootee t tttte t  ttott ttttt ttt tt oo tte t o teoe ttotttteotttott  t  tt ete teottte oe tt ttt tttotte t t ttt  t  tettto toot\n",
            "oet        e   e t  o   ett       t                       s    e   s  o   e   e     t           e     s         t               ts    t  o ts     e   to  e    o        t  t  t    \n",
            "ot     eat e   e t  t     t  a t  t                  t    t     t  a  o   ett e     t         t t     t    t    t                t    t  t  t  a  t    o       a        t  t et  t \n",
            "ot un      t   et            a t  t        a               o   tt    ea   t   t   e t   e    t  t     t   e     t e   e     o   et    t  aat  ea      ta       a        t  t   t   \n",
            "odttn e      e  d    od  e  t  dd d    d  e    e          dhe  tde   dn   t    o  t t   t    d  t e       doe t t t dht e  oh      e  t e  td t d  he t e t   dn  et    t  oh  t d \n",
            "o tt  do     dd dl   od  d  d d d d t od ed  d w do    to dhddot dt  d  eot    o  t do  ih oo o t d    o  wld t dot dht d  th   i  d  t d  t  t d  h  t d t   d   dw    to thtot   \n",
            "o too eo to  et  lo  oe     e d e d t heo t  e o to  d do dhetoo  o  do  ot t  od t do  f  to o t e    o  wl  t doo eht eo th      e  t eo o  t d  h  t e t   to  oo  o to totot   \n",
            "o to   o t   eto l   o      t   eod i ht  t    e to     o  heto l o  to  otl   o  t do  t  to   t e       wl    t t tht e  th       o t eo    t t  he to  to  to  o   o to the t   \n",
            "o toa toat   tta t   ot  t  t ett t  e to t      toa t  o dh toa eor tol  tl   he o to  th toe  t t   t   wt    toa tht    th  o d  l t ta tl t t  o  ta   o  to  ht h eto to  t e \n",
            "tetoattol  e taa t  tot     t etu th t wo teoeo atoe ehto tottoe eos toa  ttt eoe t th  t  t e  t t   tot ttt   to  t t e  toe o toe  tottaet tht to ttae ta  to   t  oeto toettte \n",
            "tetoa to   t tlett  aot  i  toe t the  wleteett atoe    o tottoe et  toal ttt doe t the t et e  tot  stot wtt   tut t e    toet  toe  tottee  tot to  ta  ta  to  tt to tt toe t e \n",
            "t toa toan theluen  tot  tn toa teto i tl lnetttatoe    o tontoe eto tont toe ton t to  i ttoe  tot   tot tna   tut toe  o toe e to   tottte  t t to  toi et  to  tt toelt toe t n \n",
            "t ton tonl tontunn  ton itn t  lt do   to teoet etoe e    to ton  on tonl ton t elt do     toe  t     ton don   tue ton    to  o t eh t  tt l tot t  meo  et  to     t  to to  t el\n",
            "t too tonl eo tuon  ton itn to lt do   to tao lonton  h o to to leon tonl ton toelt do  t  to   to    ton won   tue ton e  to  o toeh to to   tot t  leoe et  to  s  t  lt to  tonl\n",
            "t too tonl eo tuenl ton ipn to lt do i to teo lonton t  r to tonloe  wonl ton woelt do  p  to   to    ton won , tue ton em toslo to m tontosd to kth meoe eo  woh t  p  co to  tonl\n",
            "tstoo tonp to tunn  tod eps to lt eh   to peo lerton eh r to lo  eon tonl ths todlt do  p  to   to  , ton tone, tut ton em tonlh th r to lon  to  th meoe er  toh rs t  cd to  tosl\n",
            "tstoo tonp tos'u le todkeps to lt to m toetereledto  th r to 'oslee  wonp eod todtt do  r  to   to k, tou wodk, tu  to  em toslo toer to tosl to kth mereket  tohers t  tu to  t sl\n",
            "tstos tonp to butle todketo ton't do t to teo badtod t  r to totletl toot ern todlt da  t  toem tosk, tod wodk, tut to  er toslo toem tosbosl to kth  ereker  dohern t  tu to mt s'\n",
            "t,tot tonp to cutde tod ems tonlt to l to peo le ton t  m th co leao toot wos todlt dn  t  t    to  , ton wooe, tut ta  er tonlo them to butd to  to  eot er  wohetn ty tu to  t  l\n",
            "p ton todt to cutde tod im, tonlt to t da leo le ton t  m to to lead tont tns to lt tn  s  th m ton , ton aod , but tot er to co toem to co l to  th  eo keh  ao  rn ty tt to  th l\n",
            "p,ton wood tonbutde tod ip, tonlt do t da leodlo to  th m toncotlend wond tno todlt dn  r  toem tonk, bnn aon , but tother tonco toem tonconl tonktoe eonleo  woh rn ty tt toe ehnl\n",
            "phtos wond toncutle tod im, tonlt to i tn leo me to  t er to conleto tondetno wodlt tn  g  them tonk, tod won , but tonhe, tonlo toem toncutl tonhtoe eodhes  woh rs ty tf toe eh l\n",
            "p,ton tond tosbutld tod ip, tonlt to t dp beodlo to  ther tonbonlend tond tno wodlt dn  g, them tonk, tnd woo , but donher tonlo toem tonbonl tonhtoemeodleu  toh rsity tt toemehnl\n",
            "p tos wonl to butle tnd ip, to lt do i tp beodle to  ther to bo lerd tond tnn todlt tn  gs them to  s tnd to  s but tonher tonlo toem to to l ton themendlee  aom rs ty tf toemeh l\n",
            "p tonlwonl to butle tnd ip, tonlt donl tn beodle ton ther tonbonland word wnn wonlt dn  t  them ton s tnd ton s but tonher tonbo toem tonbotl ton toemendlet  tom ss ty cf themehnl\n",
            "p ton wond to butld wnd ip, ton't dorl tp leo me tonht er th conlend tord wnd wonlt dn  t  them ton s tnd tor s but tonher thnlo toem toncotl tonktoemend es  tomes  ty of themehrl\n",
            "p ton wodd to butld tod ip, ton't drdl up teodleeto  sher to con'eod wodd wnd tonlt dn  g, them tos s and tod s but to  er tonco toem to cot' to  toemendlene tomens ty of theme ul\n",
            "p tof wodl to bu ld tod ip, to lt drdl up teo leeto  sher to to 'eod wood dnd tonlt dns g, them tos s and too s but to  er to co toem to tot' to  themendles  tomersity of theme ul\n",
            "p ton won' to cu ld wod ip, don't dnul up teotleeton sher to cof'eod wodd dnd whn't dns g  them tos s and todks but to her to co toe  to con' to  themendles  tomessity of themshn'\n",
            "p ton won' to cu ld tos ip, don't dr l up teotleeton t er to cof'eod tong dnd won't dns g  toe  ton , and wook, but tonher to co toe  to con'eto kthe endles  tomessity of the thn'\n",
            "pmton wong to cuild iod ip, don't dr l up teotleetonhther to co lend toog dnd won't dns gs the  tos s tnd wook, bui tonher to co toe  to con' to  the endless iomessity of the thn'\n",
            "pmton wodg to build iod ip, fon't dr l up teotleeton ther th bonlend wodd dnd ion't dnsigs them tos s tnd iook, bui donher th co toe  to bon' ton the endlen  iomessity of the thng\n",
            "pmton tond tonbuild wns ip, don't drrm ap peotle tonether th bollend wodd dnd don't dnsigs them tos s and wodk, but dot er th co toem to bon' don themendlens dmmensity of themtenc\n",
            "pmton wond toncuild wns ip, don't drrm ap peotle thnether th collend wodd dnd won't dnsigs them tos s and work, but dother th co toem to bon' won the endless wmmessity of themsenc\n",
            "pmton won  to cutld irs ip, don't drum up peotle tonether to collecd wood dnd don't dssign them tos s and dook, but rot er to co toem to con' ton the endlens iomensity of themseng\n",
            "p ton wodt to butld wrs ip, don't dr m up peotle th ether to bo  esd wood dnd don't dss gs them tos s and dook, but rother to co toem to lon' ton the endle s immessity of themsenc\n",
            "pmten wodd tonbutld wos ip, don't drum up peotle tonether to bollecd wood dnd don't dnsigngtoem tosks and dork, but rother toubo toem to bol' aon toe endlecs iom nsity of toemeenc\n",
            "mmten wodd tonbutld wns ip, don't drum up peotle tonether to bollecd wodd dnd don't dssign them tosks and dork, but rather to co toem to bong won themendlens immensity of themeenc\n",
            "m teu wad  to cutld ins ip, don't drum up peotle to ether to collecd wodd dnd won't dssign them tos s tnd work, but rather to ch toem to cong ion themendless immensity of themesnc\n",
            "mmteu wan  to butld wns ip, don't drum up peotle to ether to bollecs wood dnd don't dssign them tos s tnd work, but rather to ch them to bong fon the endless immensity of the eenc\n",
            "mmtoo wont to butld ans ip, don't drum up peotle tonether to collecs dord and dor't dssign them tosls tnd dork, but rather to ch toem to bong fon themendless immensity of themeent\n",
            "mmteu wo ' to build ans ip, don't drum up peotle together to bollecs wood and aor't dssign them tosks and work, but rather to ch them to bong for the endless immensity of themeenc\n",
            "mmtou wont to build a s ip, don't dram up peotle together to collens wood and won't assign them tos s and work, but rather toach them to cong fon themsndless imm nsity of themeenn\n",
            "pmtou wont to cuild a s ip, don't drum up peotle together to collecs wood and won't dssign them tosks and work, but rather toach them to cong fon themendlecs immensity of themeenn\n",
            "pmtou wont to cutld a s ip, don't drum up peotle together to collecd wood and won't dssign them tosks and work, but rather toach them to colg fon the endless immensity of themeenn\n",
            "p tou want to lutld a s ip, don't drum up people together to collecs wood and won't dss gn them tosks and work, but rather thach the  to longefon the endless immensity of the eean\n",
            "p tou want to butld a s ip, don't drum up people together to lollecs wood and don't dssign them tosks and work, but rather toach the  to long ton the endless immensity of the eeun\n",
            "p tou want to butld a ship, don't drum up people together to bollecs wood and won't dssign them tosks ans work, but rather toach them to bong for the endless immensity of the eeun\n",
            "p tou want to lutld a ship, don't arum up people together to lollecs wood and won't dssign them tosls and work, but rather toach them to long for themendless immensity of themeea.\n",
            "p tou want to lutld a ship, don't arum up people together to collecs wood and don't assign them tosks and work, but rather toach the  to long for the endless immensity of the eea.\n",
            "p tou want to butld a ship, don't drum up people together to bollecs wood and won't dssign them tosks and work, but rather thach the  to cong for the endless immensity of the sea.\n",
            "p tou want to butld a ship, don't drum up people together to collecd wood and won't dssign them tosks and work, but rather toach them to bong for the endless immensity of themsea.\n",
            "p wou want to butld a ship, dor't drum up people together to collecs wood and won't dssitn them tosks and work, but rather toach the  to bong for the endless immensity of the sea.\n",
            "p you want to cutld a ship, don't drum up people together to collecs wood and don't dssign them tosks and work, but rather thach the  to long for the endless immensity of the ser.\n",
            "p you want to lutld a ship, don't drum up people together to collecs wood and don't dssign them tosks and work, but rather thach them to long for the endless immensity of the sea.\n",
            "p you want to butld a ship, don't arum up people together to collecs wood and won't dssign them tosks and work, but rather toach them to long for the endless immensity of the seas\n",
            "p you want to butld a ship, don't drum up people together to collecs wood and won't dssign them tosks and work, but rather toach them to bong for the endless immensity of the seas\n",
            "p you want to butld a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather thach them to bong for the endless immensity of the sea.\n",
            "p you want to lutld a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather thach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather thach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather thach them to bong for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather thach them to long for the endless immensity of the eean\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather thach them to long for the endless immensity of the ssac\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach the  to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't dssign them tosks and work, but rather toach the  to long for the endless immensity of the set.\n",
            "p you want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather thach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach them to bong for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assigngthem tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't drum up people together te collect wood and don't assign them tosks and work, but rather teach them to cong for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to bong for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to cong for the endless immensity of the set.\n",
            "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "t you want to build a ship, don't drum up people together te collect wood and don't assign them tosks and work, but rather teach them to bong for the endless immensity of the sea.\n",
            "t you wont to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the sean\n",
            "t you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together te collect wood and don't assign them tosks and work, but rather thach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to lo g for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "f you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
            "p you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
            "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 단어 단위 RNN"
      ],
      "metadata": {
        "id": "rikBi1QEw1UW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "GCd0OPLVwd7n"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Repeat is the best medicine for memory\".split()\n"
      ],
      "metadata": {
        "id": "tDYhGSErxaJC"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(set(sentence))\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFn6oXOwxpkZ",
        "outputId": "7b4b7e35-2fb3-4470-b340-4c027a9b0dd0"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['for', 'Repeat', 'is', 'medicine', 'memory', 'best', 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2index = {tkn: i for i, tkn in enumerate(vocab, 1)}  # 단어에 고유한 정수 부여\n",
        "word2index['<unk>']=0"
      ],
      "metadata": {
        "id": "EiUzoIy7xrD0"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CQravBOxvi7",
        "outputId": "ab8f2e07-0817-4302-d02a-592c29a98511"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'for': 1, 'Repeat': 2, 'is': 3, 'medicine': 4, 'memory': 5, 'best': 6, 'the': 7, '<unk>': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2index['memory'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yqAy6qPYx4F9",
        "outputId": "68352e16-0bf2-41ac-972b-c6c0ccac501e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 수치화된 데이터를 단어로 바꾸기 위한 사전\n",
        "index2word = {v: k for k, v in word2index.items()}\n",
        "print(index2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuZdzorPx5iv",
        "outputId": "8cc9b5ee-d221-467a-f944-980ea85e79b2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'for', 2: 'Repeat', 3: 'is', 4: 'medicine', 5: 'memory', 6: 'best', 7: 'the', 0: '<unk>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_data(sentence, word2index):\n",
        "    encoded = [word2index[token] for token in sentence] # 각 문자를 정수로 변환. \n",
        "    input_seq, label_seq = encoded[:-1], encoded[1:] # 입력 시퀀스와 레이블 시퀀스를 분리\n",
        "    input_seq = torch.LongTensor(input_seq).unsqueeze(0) # 배치 차원 추가\n",
        "    label_seq = torch.LongTensor(label_seq).unsqueeze(0) # 배치 차원 추가\n",
        "    return input_seq, label_seq"
      ],
      "metadata": {
        "id": "P9tKXs2wx8mo"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = build_data(sentence, word2index)"
      ],
      "metadata": {
        "id": "HAC2LhY4ybM_"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BCMk842oyrvt",
        "outputId": "b9a8beee-2eb0-4ed8-ba53-03133c0992ec"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2, 3, 7, 6, 4, 1]])\n",
            "tensor([[3, 7, 6, 4, 1, 5]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
        "        super(Net, self).__init__()\n",
        "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩\n",
        "                                            embedding_dim=input_size)\n",
        "        self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉 상태의 크기 정의\n",
        "                                batch_first=batch_first)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # 1. 임베딩 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이) => (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        output = self.embedding_layer(x)\n",
        "        # 2. RNN 층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
        "        # => output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden (1, 배치 크기, 은닉층 크기)\n",
        "        output, hidden = self.rnn_layer(output)\n",
        "        # 3. 최종 출력층\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기)\n",
        "        output = self.linear(output)\n",
        "        # 4. view를 통해서 배치 차원 제거\n",
        "        # 크기변화: (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기*시퀀스 길이, 단어장 크기)\n",
        "        return output.view(-1, output.size(2))"
      ],
      "metadata": {
        "id": "MRVgjXGdywJw"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QpakhRDT03uh",
        "outputId": "641a3564-297a-442e-8404-635595899693"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼 파라미터\n",
        "vocab_size = len(word2index)  # 단어장의 크기는 임베딩 층, 최종 출력층에 사용된다. <unk> 토큰을 크기에 포함한다.\n",
        "input_size = 5  # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\n",
        "hidden_size = 20  # RNN의 은닉층 크기"
      ],
      "metadata": {
        "id": "O0LB9nCQy5aO"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 생성\n",
        "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
        "# 손실함수 정의\n",
        "loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 실제값은 원-핫 인코딩 안 해도 됨.\n",
        "# 옵티마이저 정의\n",
        "optimizer = optim.Adam(params=model.parameters())"
      ],
      "metadata": {
        "id": "J0VpyFTE0DvX"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 임의로 예측해보기. 가중치는 전부 랜덤 초기화 된 상태이다.\n",
        "output = model(X)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tn8pDy-I0IEd",
        "outputId": "ff05faeb-8326-438e-950f-e9a1be10064f"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2396, -0.2473, -0.2938, -0.3068,  0.5220, -0.0239, -0.2340,  0.0285],\n",
            "        [-0.3767, -0.1616, -0.3012, -0.0468, -0.1227,  0.3465,  0.2663,  0.0856],\n",
            "        [-0.3933,  0.1195, -0.0882,  0.2058,  0.1384,  0.3180, -0.0703, -0.0584],\n",
            "        [-0.3772, -0.0769, -0.2327,  0.2912,  0.4598, -0.1309, -0.0575,  0.2115],\n",
            "        [-0.3359,  0.0888, -0.1719,  0.3178,  0.1754, -0.0426,  0.0904, -0.0899],\n",
            "        [-0.6043,  0.0930, -0.0345,  0.2958, -0.0339,  0.0264,  0.2607, -0.1717]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output.softmax(-1).argmax(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Klei1s5g2a7v",
        "outputId": "123b326d-cd20-413a-9497-bd31c43b5f6a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([3, 7, 6, 4, 1, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode(output.softmax(-1).argmax(-1).tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8AMicIx2kj8",
        "outputId": "ab187df3-90ed-4947-bcc3-7d43d7e2a84a"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['is', 'the', 'best', 'medicine', 'for', 'memory']"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode = lambda y: [index2word.get(x) for x in y]\n"
      ],
      "metadata": {
        "id": "dZNYgfZI0JMa"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 시작\n",
        "for step in range(201):\n",
        "    # 경사 초기화\n",
        "    optimizer.zero_grad()\n",
        "    # 순방향 전파\n",
        "    output = model(X)\n",
        "    # 손실값 계산\n",
        "    loss = loss_function(output, Y.view(-1))\n",
        "    # 역방향 전파\n",
        "    loss.backward()\n",
        "    # 매개변수 업데이트\n",
        "    optimizer.step()\n",
        "    # 기록\n",
        "    if step % 40 == 0:\n",
        "        print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n",
        "        pred = output.softmax(-1).argmax(-1).tolist()\n",
        "        print(decode(pred))\n",
        "        print(\" \".join([\"Repeat\"] + decode(pred)))\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2ei5Szp1Pc0",
        "outputId": "ff8d2bd6-d39c-4275-f70c-2f9a7b9214ea"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/201] 2.1226 \n",
            "['for', 'the', 'the', 'memory', 'memory', 'memory']\n",
            "Repeat for the the memory memory memory\n",
            "\n",
            "[41/201] 1.5330 \n",
            "['is', 'the', 'best', 'memory', 'memory', 'memory']\n",
            "Repeat is the best memory memory memory\n",
            "\n",
            "[81/201] 0.9428 \n",
            "['is', 'the', 'best', 'medicine', 'for', 'memory']\n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[121/201] 0.5143 \n",
            "['is', 'the', 'best', 'medicine', 'for', 'memory']\n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[161/201] 0.2801 \n",
            "['is', 'the', 'best', 'medicine', 'for', 'memory']\n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[201/201] 0.1608 \n",
            "['is', 'the', 'best', 'medicine', 'for', 'memory']\n",
            "Repeat is the best medicine for memory\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1zrKioCl1TR3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}